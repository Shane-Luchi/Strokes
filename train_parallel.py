import torch
from datasets import Dataset
from modelscope import snapshot_download, AutoTokenizer
from qwen_vl_utils import process_vision_info
from transformers import (
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    Qwen2VLForConditionalGeneration,
    AutoProcessor,
)
import json
import time
import os
from nltk.translate.bleu_score import sentence_bleu
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.distributed as dist


# Setup logging with TensorBoard
summarywriter = SummaryWriter(log_dir="./logs")
import torch.distributed as dist

# åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„
dist.init_process_group(backend='nccl', init_method='env://')

# è·å–å½“å‰è¿›ç¨‹çš„ local_rank å’Œè®¾å¤‡
local_rank = int(os.environ['LOCAL_RANK'])
print("local_rank:", local_rank)
torch.cuda.set_device(local_rank)

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ŒæŒ‡å®šGPUè®¾å¤‡
os.environ["CUDA_VISIBLE_DEVICES"] = "2,3"  # ä½¿ç”¨ GPU 2 å’Œ GPU 3

# ====================== æ–°å¢EOSå®šä¹‰ ======================
EOS_TOKEN = "<EOS>"

# åŠ è½½ tokenizer å’Œ processor
tokenizer = AutoTokenizer.from_pretrained(
    "/home/LLMs/Qwen/Qwen2-VL-2B-Instruct/", use_fast=False, trust_remote_code=True
)
processor = AutoProcessor.from_pretrained("/home/LLMs/Qwen/Qwen2-VL-2B-Instruct")

# æ·»åŠ  EOS Token åˆ° tokenizer
tokenizer.add_special_tokens({'eos_token': EOS_TOKEN})
EOS_TOKEN_ID = tokenizer.eos_token_id

# åŠ è½½æ¨¡å‹
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "/home/LLMs/Qwen/Qwen2-VL-2B-Instruct/",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
)
model.resize_token_embeddings(len(tokenizer))  # ğŸš¨ è°ƒæ•´Embeddingä»¥é€‚é…æ–°token

# å°†æ¨¡å‹ç§»åŠ¨åˆ°å½“å‰è®¾å¤‡ï¼ˆGPUï¼‰
model = model.to(local_rank)

model = DDP(model, device_ids=[local_rank])  # å‡è®¾ä½¿ç”¨ä¸¤ä¸ªGPU
# ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åŒæ­¥
dist.barrier()

# ===================== å¤„ç†å‡½æ•° =====================
def process_func(example):
    MAX_LENGTH = 8192
    conversation = example["conversations"]
    input_content = conversation[0]["value"]
    output_content = conversation[1]["value"]
    file_path = input_content.split("<|vision_start|>")[1].split("<|vision_end|>")[0]
    file_path = f"data/{file_path}"
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": f"{file_path}",
                    "resized_height": 280,
                    "resized_width": 280,
                },
                {"type": "text", "text": "è¯·ä½ ç”¨ä¸­æ–‡æè¿°å›¾ç‰‡ä¸­æ±‰å­—çš„ç¬”ç”»é¡ºåº"},
            ],
        }
    ]
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    )
    inputs = {key: value.tolist() for key, value in inputs.items()}
    instruction = inputs

    # ======= æ·»åŠ EOS Tokenåˆ°æ ‡ç­¾æœ«å°¾ ========
    response = tokenizer(f"{output_content}{EOS_TOKEN}", add_special_tokens=False)

    input_ids = instruction["input_ids"][0] + response["input_ids"] + [tokenizer.pad_token_id]
    attention_mask = instruction["attention_mask"][0] + response["attention_mask"] + [1]
    labels = [-100] * len(instruction["input_ids"][0]) + response["input_ids"] + [tokenizer.pad_token_id]

    if len(input_ids) > MAX_LENGTH:
        input_ids = input_ids[:MAX_LENGTH]
        attention_mask = attention_mask[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH]

    vocab_size = tokenizer.vocab_size
    labels = [label if label == -100 or (0 <= label < vocab_size) else -100 for label in labels]

    input_ids = torch.tensor(input_ids)
    attention_mask = torch.tensor(attention_mask)
    labels = torch.tensor(labels)
    inputs["pixel_values"] = torch.tensor(inputs["pixel_values"])
    inputs["image_grid_thw"] = torch.tensor(inputs["image_grid_thw"]).squeeze(0)

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels,
        "pixel_values": inputs["pixel_values"],
        "image_grid_thw": inputs["image_grid_thw"],
    }

# ===================== æ¨ç†å‡½æ•° =====================
def predict(messages, model, max_new_tokens=64, temperature=1.0, top_p=0.9, top_k=50):
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    )
    inputs = inputs.to("cuda")

    generated_ids = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        eos_token_id=EOS_TOKEN_ID,    # æŒ‡å®šç»ˆæ­¢ç¬¦å·
        pad_token_id=tokenizer.pad_token_id,
        temperature=temperature,      # æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ï¼Œå¤šæ ·æ€§
        top_p=top_p,                  # nucleus sampling
        top_k=top_k                   # é™åˆ¶é‡‡æ ·èŒƒå›´
    )

    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False,
    )
    return output_text[0]

# ===================== è‡ªå®šä¹‰ Trainer =====================
class CustomTrainer(Trainer):
    def __init__(self, *args, summarywriter=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.summarywriter = summarywriter  # å°†tb_writerä¼ é€’ç»™Trainerç±»
    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset
        if eval_dataset is None:
            raise ValueError("Trainer: evaluation requires an eval_dataset.")

        self.model.eval()
        total_samples = len(eval_dataset)
        correct_samples = 0

        for example in eval_dataset:
            true_output = example["conversations"][1]["value"]
            input_content = example["conversations"][0]["value"]
            file_path = input_content.split("<|vision_start|>")[1].split("<|vision_end|>")[0]
            file_path = f"data/{file_path}"
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "image": f"{file_path}",
                            "resized_height": 280,
                            "resized_width": 280,
                        },
                        {"type": "text", "text": "è¯·ä½ ç”¨ä¸­æ–‡æè¿°å›¾ç‰‡ä¸­æ±‰å­—çš„ç¬”ç”»é¡ºåº"},
                    ],
                }
            ]
            pred_output = predict(messages, self.model)
            print(f"Predicted: {pred_output.strip()}")
            print(f"True     : {true_output.strip()}")
            if pred_output.strip() == true_output.strip():
                correct_samples += 1
            bleu_score = sentence_bleu([true_output.split()], pred_output.split())
            print(f"BLEU Score: {bleu_score}")

        accuracy = correct_samples / total_samples if total_samples > 0 else 0
        print(f"Validation Accuracy: {accuracy:.4f} ({correct_samples}/{total_samples})")
        metrics = super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
        metrics["eval_accuracy"] = accuracy

        # è®°å½•è¯„ä¼°ç»“æœåˆ° TensorBoard
        summarywriter.add_scalar('eval/accuracy', accuracy, self.state.global_step)

        return metrics

    def training_step(self, model, inputs):
        # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
        loss = super().training_step(model, inputs)

        # è·å–å½“å‰epochçš„lossï¼Œè®°å½•åˆ°TensorBoard
        summarywriter.add_scalar('train/loss', loss.item(), self.state.global_step)
        
        return loss

# # ===================== åŠ è½½æ•°æ® =====================
# with open("data/train_data.json", "r") as f:
#     data = json.load(f)

# total_size = len(data)
# train_size = int(total_size * 0.97)
# val_size = int(total_size * 0.02)
# test_size = total_size - train_size - val_size

# train_data = data[:train_size]
# val_data = data[train_size:train_size + val_size]
# test_data = data[train_size + val_size:]

# extra_indices = [i for i in range(0, 8105, 100)]
# extra_data = [data[i] for i in extra_indices]
# # å°† extra_data æ”¾åœ¨ val_data çš„å‰é¢
# val_data = extra_data + val_data

# with open("data/data_vl_train.json", "w") as f:
#     json.dump(train_data, f)
# with open("data/data_vl_val.json", "w") as f:
#     json.dump(val_data, f)
# with open("data/data_vl_test.json", "w") as f:
#     json.dump(test_data, f)

# åŠ è½½å¤„ç†åæ•°æ®
from datasets import load_from_disk
train_dataset = load_from_disk("data/train_dataset_processed")
val_dataset = load_from_disk("data/val_dataset_processed")

# ===================== è®­ç»ƒå‚æ•° =====================
args = TrainingArguments(
    output_dir="./output3/Qwen2-VL-2B",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    logging_steps=10,
    logging_dir="./logs",
    num_train_epochs=50,
    save_steps=100,
    learning_rate=1e-4,
    lr_scheduler_type="cosine",
    warmup_steps=2000,
    save_on_each_node=True,
    gradient_checkpointing=True,
    report_to="tensorboard",
    ddp_find_unused_parameters=False,  # è®¾ç½®åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ
    bf16=True,
    dataloader_num_workers=8,
    evaluation_strategy="epoch",
    per_device_eval_batch_size=4,
    local_rank=-1,  # ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒ
)

# ä¸å†ä½¿ç”¨ LoRA é…ç½®ï¼Œç›´æ¥è¿›è¡Œå…¨é‡å¾®è°ƒ
trainer = CustomTrainer(
    model=model,
    args=args,
    summarywriter = summarywriter,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
)

print("--------------starting to train---------------")
trainer.train()

# ===================== æµ‹è¯•æ¨¡å‹ =====================
with open("data/data_vl_test.json", "r") as f:
    test_dataset = json.load(f)

for item in test_dataset:
    input_image_prompt = item["conversations"][0]["value"]
    origin_image_path = input_image_prompt.split("<|vision_start|>")[1].split("<|vision_end|>")[0]
    origin_image_path = f"data/{origin_image_path}"
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": origin_image_path},
                {"type": "text", "text": "è¯·ä½ ç”¨ä¸­æ–‡æè¿°å›¾ç‰‡ä¸­æ±‰å­—çš„ç¬”ç”»é¡ºåº"},
            ],
        }
    ]
    response = predict(messages, model)  # ä½¿ç”¨å…¨é‡å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
    messages.append({"role": "assistant", "content": f"{response}"})
    print(messages[-1])